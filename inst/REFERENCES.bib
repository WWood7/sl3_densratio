@article{vdl2007super,
  title={Super learner},
  author={{van der Laan}, Mark J and Polley, Eric C and Hubbard, Alan E},
  journal={Statistical applications in genetics and molecular biology},
  volume={6},
  number={1},
  year={2007}
}

@article{breiman1996stacked,
  title={Stacked regressions},
  author={Breiman, Leo},
  journal={Machine learning},
  volume={24},
  number={1},
  pages={49--64},
  year={1996},
  publisher={Springer}
}

@article{wolpert1992stacked,
  title={Stacked generalization},
  author={Wolpert, David H},
  journal={Neural networks},
  volume={5},
  number={2},
  pages={241--259},
  year={1992},
  publisher={Elsevier}
}

@article{friedman1991multivariate,
  title={Multivariate adaptive regression splines},
  author={Friedman, Jerome H},
  journal={The Annals of Statistics},
  pages={1--67},
  year={1991},
  publisher={JSTOR}
}

@techreport{friedman1993fast,
  title={Fast {MARS}},
  author={Friedman, Jerome H},
  year={1993},
  institution={Stanford University},
  url={https://statistics.stanford.edu/sites/g/files/sbiybj6031/f/LCS%20110.pdf}
}

@Article{bartMachine,
  title = {{bartMachine}: Machine Learning with {B}ayesian Additive
    Regression Trees},
  author = {Adam Kapelner and Justin Bleich},
  journal = {Journal of Statistical Software},
  year = {2016},
  volume = {70},
  number = {4},
  pages = {1--40},
  doi = {10.18637/jss.v070.i04},
}

@Article{ranger,
  title = {{ranger}: A Fast Implementation of Random Forests for High
    Dimensional Data in {C++} and {R}},
  author = {Marvin N. Wright and Andreas Ziegler},
  journal = {Journal of Statistical Software},
  year = {2017},
  volume = {77},
  number = {1},
  pages = {1--17},
  doi = {10.18637/jss.v077.i01},
}

@article{glmnet,
  title={Regularization paths for generalized linear models via coordinate
    descent},
  author={Friedman, Jerome and Hastie, Trevor and Tibshirani, Rob},
  journal={Journal of statistical software},
  volume={33},
  number={1},
  pages={1},
  year={2010},
  publisher={NIH Public Access}
}

@inproceedings{xgboost,
  title={Xgboost: A scalable tree boosting system},
  author={Chen, Tianqi and Guestrin, Carlos},
  booktitle={Proceedings of the 22nd {ACM SIGKDD} international conference on
    knowledge discovery and data mining},
  pages={785--794},
  year={2016}
}

@article{speedglm,
  title={Fitting linear models and generalized linear models with large data
    sets in {R}},
  author={Enea, Marco},
  journal={Statistical Methods for the Analysis of Large Datasets: book of
    short papers},
  pages={411--414},
  year={2009}
}

@book{mgcv,
  title={Generalized additive models: an introduction with {R}},
  author={Wood, Simon N},
  year={2017},
  publisher={CRC press}
}

@book{hastie-gams,
  title={Generalized additive models},
  author={Hastie, Trevor J and Tibshirani, Robert J},
  volume={43},
  year={1990},
  publisher={CRC press}
}

@article{friedman-gbm1,
  title={Greedy function approximation: a gradient boosting machine},
  author={Friedman, Jerome H},
  journal={Annals of statistics},
  pages={1189--1232},
  year={2001},
  publisher={JSTOR}
}

@article{friedman-gbm2,
  title={Stochastic gradient boosting},
  author={Friedman, Jerome H},
  journal={Computational statistics \& data analysis},
  volume={38},
  number={4},
  pages={367--378},
  year={2002},
  publisher={Elsevier}
}

@inproceedings{lightgbm,
  author = {Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and
    Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
  booktitle = {Advances in Neural Information Processing Systems},
  pages={3146--3154},
  title = {{LightGBM}: A Highly Efficient Gradient Boosting Decision Tree},
  volume = {30},
  year = {2017}
}

@inproceedings{benkeser2016hal,
  doi = {10.1109/dsaa.2016.93},
  url = {https://doi.org/10.1109/dsaa.2016.93},
  year  = {2016},
  publisher = {{IEEE}},
  author = {Benkeser, David and {van der Laan}, Mark J},
  title = {The Highly Adaptive Lasso Estimator},
  booktitle = {2016 {IEEE} International Conference on Data Science and
    Advanced Analytics ({DSAA})}
}

@manual{coyle2020hal9001-rpkg,
  author = {Coyle, Jeremy R and Hejazi, Nima S and {van der Laan}, Mark J},
  title = {{hal9001}: The scalable highly adaptive lasso},
  year  = {2020},
  url = {https://doi.org/10.5281/zenodo.3558313},
  doi = {10.5281/zenodo.3558313},
  note = {{R} package version 0.2.7}
}

@article{hejazi2020hal9001-joss,
  author = {Hejazi, Nima S and Coyle, Jeremy R and {van der Laan}, Mark J},
  title = {{hal9001}: Scalable highly adaptive lasso regression in {R}},
  year  = {2020},
  url = {https://doi.org/10.21105/joss.02526},
  doi = {10.21105/joss.02526},
  journal = {Journal of Open Source Software},
  publisher = {The Open Journal}
}

@manual{forecast,
  title = {{forecast}: Forecasting functions for time series and linear
    models},
  author = {Rob Hyndman and George Athanasopoulos and Christoph Bergmeir and
    Gabriel Caceres and Leanne Chhay and Mitchell O'Hara-Wild and Fotios
    Petropoulos and Slava Razbash and Earo Wang and Farah Yasmeen},
  year = {2021},
  note = {R package version 8.14},
  url = {https://pkg.robjhyndman.com/forecast/},
}

@article{hyndman2008forecast-jss,
  title = {Automatic time series forecasting: the forecast package for {R}},
  author = {Rob J Hyndman and Yeasmin Khandakar},
  journal = {Journal of Statistical Software},
  volume = {26},
  number = {3},
  pages = {1--22},
  year = {2008},
  url = {https://www.jstatsoft.org/article/view/v027i03},
}

@manual{e1071,
  title = {e1071: Misc Functions of the Department of Statistics, Probability
    Theory Group (Formerly: E1071), TU Wien},
  author = {David Meyer and Evgenia Dimitriadou and Kurt Hornik and Andreas
    Weingessel and Friedrich Leisch},
  year = {2021},
  note = {R package version 1.7-6},
  url = {https://CRAN.R-project.org/package=e1071},
}

@article{libsvm,
  author = {Chang, Chih-Chung and Lin, Chih-Jen},
  title = {{LIBSVM}: A library for support vector machines},
  journal = {ACM Transactions on Intelligent Systems and Technology},
  volume = {2},
  issue = {3},
  year = {2011},
  pages = {27:1--27:27},
  note = {Software available at \url{http://www.csie.ntu.edu.tw/~cjlin/libsvm}}
}


@inproceedings{sugiyama_direct_2007,
	title = {Direct Importance Estimation with Model Selection and Its Application to Covariate Shift Adaptation},
	volume = {20},
	url = {https://papers.nips.cc/paper_files/paper/2007/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract.html},
	abstract = {When training and test samples follow different input distributions (i.e., the situation called {\textbackslash}emph\{covariate shift\}), the maximum likelihood estimator is known to lose its consistency. For regaining consistency, the log-likelihood terms need to be weighted according to the {\textbackslash}emph\{importance\} (i.e., the ratio of test and training input densities). Thus, accurately estimating the importance is one of the key tasks in covariate shift adaptation. A naive approach is to first estimate training and test input densities and then estimate the importance by the ratio of the density estimates. However, since density estimation is a hard problem, this approach tends to perform poorly especially in high dimensional cases. In this paper, we propose a direct importance estimation method that does not require the input density estimates. Our method is equipped with a natural model selection procedure so tuning parameters such as the kernel width can be objectively optimized. This is an advantage over a recently developed method of direct importance estimation. Simulations illustrate the usefulness of our approach.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Sugiyama, Masashi and Nakajima, Shinichi and Kashima, Hisashi and Buenau, Paul and Kawanabe, Motoaki},
	urldate = {2023-05-08},
	date = {2007},
	file = {Full Text PDF:/Users/winnwu/Zotero/storage/PPXMIXMJ/Sugiyama et al. - 2007 - Direct Importance Estimation with Model Selection .pdf:application/pdf},
}

@article{kanamori_least-squares_2009,
	title = {A Least-squares Approach to Direct Importance Estimation},
	volume = {10},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v10/kanamori09a.html},
	pages = {1391--1445},
	number = {48},
	journaltitle = {Journal of Machine Learning Research},
	author = {Kanamori, Takafumi and Hido, Shohei and Sugiyama, Masashi},
	urldate = {2023-05-08},
	date = {2009},
	langid = {english},
	file = {Full Text PDF:/Users/winnwu/Zotero/storage/A5ZHKBHF/Kanamori et al. - 2009 - A Least-squares Approach to Direct Importance Esti.pdf:application/pdf},
}

@misc{wu_density_2024,
	title = {A Density Ratio Super Learner},
	url = {http://arxiv.org/abs/2408.04796},
	doi = {10.48550/arXiv.2408.04796},
	abstract = {The estimation of the ratio of two density probability functions is of great interest in many statistics fields, including causal inference. In this study, we develop an ensemble estimator of density ratios with a novel loss function based on super learning. We show that this novel loss function is qualified for building super learners. Two simulations corresponding to mediation analysis and longitudinal modified treatment policy in causal inference, where density ratios are nuisance parameters, are conducted to show our density ratio super learner's performance empirically.},
	number = {{arXiv}:2408.04796},
	publisher = {{arXiv}},
	author = {Wu, Wencheng and Benkeser, David},
	urldate = {2024-08-17},
	date = {2024-08-08},
	eprinttype = {arxiv},
	eprint = {2408.04796 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/winnwu/Zotero/storage/GRNWL72E/Wu and Benkeser - 2024 - A Density Ratio Super Learner.pdf:application/pdf;arXiv.org Snapshot:/Users/winnwu/Zotero/storage/23TQT7TX/2408.html:text/html},
}
