---
title: "Density Ratio Learners in `sl3`"
author: "Wencheng Wu"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
bibliography: ../inst/REFERENCES.bib
vignette: >
  %\VignetteIndexEntry{Modern Machine Learning in R}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

<!-- this r section is for local test, have to be modified in final version-->
```{r setup, echo=FALSE, results='hide', message=FALSE}
setwd("/Users/winnwu/Documents/GitHub/sl3_densratio")
devtools::load_all()
set.seed(49753)
data(cpp_imputed)
```

## Introduction

Ratios of different probability density functions can be useful to estimate in some scenarios. 
In causal inference, density ratios can be of indirect interests as some nuisance parameters 
incorporated in estimands or estimators. In general machine learning, density ratios can be useful 
in addressing covariates shift adaptation and outlier detection. `sl3` is equipped with learners that 
can tackle density ratios written in the form $\frac{f(X)}{g(X)}$ or $\frac{f(X\mid Y)}{g(X\mid Y)}$.

Different from situations like classification and regression, where 
the true values of the variables of interest is included in the training data, in density ratio 
estimation we cannot observe the actual density ratios. Thus, `sl3` handles density ratio 
estimation differently. This guide describes the process of implementing density ratio learners in `sl3`
to estimate density ratios. 

### Example Data

<!-- use `smoked` as the binary variable -->

Throughout this vignette, we use data from the Collaborative Perinatal Project
(CPP) to illustrate the features of `sl3` density ratio learners. For
convenience, we've included an imputed version of this dataset in the `sl3`
package. Below, we load some useful packages, and load the `cpp_imputed` dataset:


<!-- have to be modified for final version -->
```{r, eval=FALSE}
set.seed(49753)

# packages we'll be using
library(sl3)

# load example data set
data(cpp_imputed)
```

We include three variables in our guide: continuous variables `birthwt`, `birthlen`, 
and a binary variable `smoked`. 

```{r}
variables <- c("birthwt", "birthlen", "smoked")
head(cpp_imputed[variables])
```

For convenience, we use $W$ to denote `birthwt`, $L$ to denote `birthlen`, and 
$S$ to denote `smoked`. Say we want to estimate $\frac{p(W,L\mid S=1)}{p(W,L\mid S=0)}$ 
and $\frac{p(W\mid L, S=1)}{p(W\mid L, S=0)}$, suppose $p$ is the true PDF. Given a reasonable 
value of $(W,L)$, say $(w,l)$, our final estimates should be able to give estimated values of 
$\frac{p(w,l\mid S=1)}{p(w,l\mid S=0)}$ and $\frac{p(w\mid l, S=1)}{p(w\mid l, S=0)}$.

Here we see $S$ as an indicator of two different marginal distributions, i.e., $p(W,L\mid S=s)$ with different `s` 
are different marginal distributions of $(W,L)$. Therefore we call density ratios with similar forms to 
$\frac{p(W,L\mid S=1)}{p(W,L\mid S=0)}$ marginal density ratios. Density ratios with forms as $\frac{p(W\mid L, S=1)}{p(W\mid L, S=0)}$ will be termed as conditional density ratios.

## Estimation Process

### Definint the Task

We now start our estimation process. To implement any learner in `sl3`, we need an `sl3_Task` object 
in which covariates and an outcome is assigned. In our case, the covariates are `birthwt` and `birthlen`, 
the tricky part is that we set `smoked` as the outcome. 

The point here is, the variable we select as outcome is an indicator that indicates the source distributions 
of the observed covariates. In a `sl3_Task` object for density ratio estimation, 
the outcome variable should always be a 0-1 variable. Value 1 indicates the numerator distribution and value 0 
indicates the denominator distribution. In our case, the variable `smoked` is just a perfect indicator against our needs. 
However, in practice, one may need to merge or subset datasets and manually create a 0-1 variable as the indicator.


```{r}
# define the task object
task <- sl3_Task$new(data = cpp_imputed, 
                     covariates = c('birthwt', 'birthlen'), 
                     outcome = 'smoked')
```

### Learners for Marginal Density Ratio

`sl3` has two types of learners that can handle density ratios, the kernel-based ones and the classification-based ones, see[ref].  
Kernel-based learners can be defined with three different values of a `method` argument, corresponding to three different kernel 
methods, see [ref].

```{r}
# define a kernel-based learner
klrnr <- Lrnr_densratio_kernel$new(method = 'RuLSIF', kernel_num = 200, alpha = 0.5)
```

Similarly, to build classification-based learners, we need to determine the classification algorithms we want to use. There is a 
`classifier` argument for assigning a defined `sl3` classification learner to our classification-based density ratio learner.

```{r}
#define a classification-based learner
clrnr <- Lrnr_densratio_classification$new(classifier = make_learner(Lrnr_gam))
```

Now we use the classification-based learner to run the estimation. The process with a kernel-based learner should 
be no different. However, since the kernel methods use squared Euclidean distance among data points to do the estimation, 
it is highly recommended to standardize the data first if one aims to use kernel-based learners.

```{r}
# fit learners to task data
clrnr_fit <- clrnr$train(task)

# get predictions
clrnr_preds <- clrnr_fit$predict(task)
head(clrnr_preds)
```

Our learner is able to generate predictions.

### Estimating Conditional Density Ratios

The conditional density ratio can be written as the ratio of two marginal density ratios: $\frac{p(W\mid L, S=1)}{p(W\mid L, S=0)}=\frac{p(W,L\mid S=1)}{p(W,L\mid S=0)}\Big/\frac{p(L\mid S=1)}{p(L\mid S=0)}$. That is how `sl3` can tackle the conditional density ratio, by conducting two marginal density 
ratios. To do this, we need to compose two density ratio learners using `Pipeline`.

```{r}

```


In addition to the simple usage demonstrated above, `make_sl3_Task` supports a
range of options in order to facilitate the proper articulation of more advanced
specifics potentially informative of the machine learning problem of interest.
For example, we can specify the `id`, `weights`, and `offset` nodes listed
above. These additional features are documented in the help for
[`sl3_Task`](https://sl3.tlverse.org/reference/sl3_Task.html).

## Learners

`Lrnr_base` is the base class for defining machine learning algorithms, as well
as _fits_ for those algorithms to particular `sl3_Task`s. Different machine
learning algorithms are defined in classes that inherit from `Lrnr_base`. For
instance, the `Lrnr_glm` class inherits from `Lrnr_base`, and defines a learner
that fits generalized linear models. We will use the term _learners_ to refer to
the family of classes that inherit from `Lrnr_base`. Learner objects can be
constructed from their class definitions using the `make_learner` function:

```{r sl3-make_learner}
# make learner object
lrnr_glm <- make_learner(Lrnr_glm)
```

Because all learners inherit from `Lrnr_base`, they have many features in
common, and can be used interchangeably. All learners define three main methods:
`train`, `predict`, and `chain`. The first, `train`, takes a `sl3_task` object,
and returns a learner_fit, which has the same class as the learner that was
trained:

```{r sl3-learner-train}
# fit learner to task data
lrnr_glm_fit <- lrnr_glm$train(task)

# verify that the learner is fit
lrnr_glm_fit$is_trained
```

Here, we fit the learner to the CPP task we defined above. Both `lrnr_glm` and
`lrnr_glm_fit` are objects of class `Lrnr_glm`, although the former defines a
learner and the latter defines a fit of that learner. We can distiguish between
the learners and learner fits using the `is_trained` field, which is true for
fits but not for learners.

Now that we've fit a learner, we can generate predictions using the `predict`
method:

```{r sl3-learner-predict}
# get learner predictions
preds <- lrnr_glm_fit$predict(task)
head(preds)
```

Here, we specified `task` as the task for which we wanted to generate
predictions. If we had omitted this, we would have gotten the same predictions
because `predict` defaults to using the task provided to `train` (called the
training task). Alternatively, we could have provided a different task for which
we want to generate predictions.

The final important learner method, `chain`, will be discussed below, in the
section on __learner composition__. As with `sl3_Task`, learners have a variety
of fields and methods we haven't discussed here. More information on these is
available in the help for [`Lrnr_base`](https://sl3.tlverse.org/reference/Lrnr_base.html).

### Finding Learners

Learners have _properties_ that indicate what features they support. You can use
`sl3_list_properties` to get a list of all properties supported by at least one
learner. You can then use `sl3_list_learners` to find learners supporting any
set of properties. For example:

```{r sl3-list-learner}
sl3_list_properties()

sl3_list_learners(c("binomial", "offset"))
```

The list of supported learners is currently somewhat limited. Despite current
limitations, some learners not yet supported natively in `sl3` can be used via
their corresponding wrappers in the `SuperLearner` package. `SuperLearner`
wrappers, screeners, and methods can all be used as `sl3` learners via
`Lrnr_pkg_SuperLearner`, `Lrnr_pkg_SuperLearner_screener`, and
`Lrnr_pkg_SuperLearner_method` respectively. To learn more about `SuperLearner`
wrappers, screeners, and methods, consult the documentation provided with that
R package. Here's an example of defining a `sl3` learner that uses the
`SL.glmnet` wrapper from `SuperLearner`.

```{r SuperLearner Wrapper}
lrnr_sl_glmnet <- make_learner(Lrnr_pkg_SuperLearner, "SL.glmnet")
```

In most cases, using these wrappers will not be as efficient as their native
`sl3` counterparts. If your favorite learner is missing from `sl3`, please
consider adding it by following the ["Defining New
Learners"](custom_lrnrs.html) vignette.

### Learner Parameters

In general, learners can be instantiated without providing any additional
parameters. We've tried to provide sensible defaults for each learner; however,
if you would like to modify the learners' behavior, you may do so by
instantiating learners with different parameters.

`sl3` Learners support some common parameters that work with all learners for
which they are applicable:

* `covariates`: subsets covariates before fitting. This allows different
  learners to be fit to the same task with different covariate subsets.

* `outcome_type`: overrides the `task$outcome_type`. This allows different
  learners to be fit to the same task with different outcome_types.

* `...`: abitrary parameters typically passed directly to the internal learner
  method. The documentation for each learner will direct to the appropriate
  function documentation for the learner method.

## Composing Learners

`sl3` defines two special learners, `Pipeline` and `Stack`, that allow learners
to be composed in a flexible manner.

### Pipelines

A pipeline is a set of learners to be fit _sequentially_, where the fit from one
learner is used to define the task for the next learner. There are many ways in
which a learner can define the task for the downstream learner. The `chain`
method defined by learners defines how this will work. Let's look at the example
of pre-screening variables. For now, we'll rely on a screener from the
`SuperLearner` package, although native `sl3` screening algorithms will be
implemented soon.

Below, we generate a screener object based on the `SuperLearner` function
`screen.corP` and fit it to our task. Inspecting the fit, we see that it
selected a subset of covariates:

```{r sl3-fit-screener, message=FALSE}
screen_cor <- Lrnr_pkg_SuperLearner_screener$new("screen.corP")
screen_fit <- screen_cor$train(task)
print(screen_fit)
```

Now, `chain` may be called on this learner fit to define a downstream task:

```{r sl3-chain-screener}
screened_task <- screen_fit$chain()
print(screened_task)
```

As with `predict`, we can omit a task from the call to `chain`, in which case
the call defaults to using the same task that was used for training. We can see
that the chained task reduces the covariates to the subset selected by the
screener. We can fit this new task using the `lrnr_glm` we defined above:

```{r sl3-glm-on-screened}
screened_glm_fit <- lrnr_glm$train(screened_task)
screened_preds <- screened_glm_fit$predict()
head(screened_preds)
```

The `Pipeline` class automates this process. It takes an arbitrary number of
learners and fits them sequentially, training and chaining each one in turn.
Since `Pipeline` is a learner like any other, it shares the same interface. We
can define a pipeline using `make_learner`, and use `train` and `predict` just
as we did before:

```{r sl3-define-pipeline}
sg_pipeline <- make_learner(Pipeline, screen_cor, lrnr_glm)
sg_pipeline_fit <- sg_pipeline$train(task)
sg_pipeline_preds <- sg_pipeline_fit$predict()
head(sg_pipeline_preds)
```

We see that the pipeline returns the same predictions as manually training `glm`
on the chained task from the screening learner.

We can visualize the pipeline we defined above:

```{r sl3-viz-pipeline, echo=FALSE}
dt <- delayed_learner_train(sg_pipeline, task)
plot(dt, color=FALSE, height="300px")
```

### Stacks

Like `Pipeline`s, `Stack`s combine multiple learners. `Stack`s train learners
_simultaneously_, so that their predictions can be either combined or compared.
Again, `Stack` is just a special learner and so has the same interface as all
other learners:

```{r sl3-stack}
stack <- make_learner(Stack, lrnr_glm, sg_pipeline)
stack_fit <- stack$train(task)
stack_preds <- stack_fit$predict()
head(stack_preds)
```

Above, we've defined and fit a `stack` comprised of a simple `glm` learner as
well as a pipeline that combines a screening algorithm with that same learner.
We could have included any abitrary set of learners and pipelines, the latter of
which are themselves just learners. We can see that the `predict` method now
returns a matrix, with a column for each learner included in the stack.

We can visualize the stack:


```{r sl3-viz-stack, echo=FALSE}
dt <- delayed_learner_train(stack, task)
plot(dt, color=FALSE, height="500px")
```

We see one "branch" for each learner in the stack.

### Cross-validation

Having defined a stack, we might want to compare the performance of learners in
the stack, which we may do using _cross-validation_. The `Lrnr_cv` learner wraps
another learner and performs training and prediction in a cross-validated
fashion, using separate training and validation splits as defined by
`task$folds`.

Below, we define a new `Lrnr_cv` object based on the previously defined `stack`
and train it and generate predictions on the validation set:

```{r sl3-cv-stack}
cv_stack <- Lrnr_cv$new(stack)
cv_fit <- cv_stack$train(task)
cv_preds <- cv_fit$predict()
```

We can also use the special `Lrnr_cv` function `cv_risk` to estimate
cross-validated risk values:

```{r sl3-cv-risk}
risks <- cv_fit$cv_risk(loss_squared_error)
print(risks)
```

In this example, we don't see much difference between the two learners,
suggesting the addition of the screening step in the pipeline learner didn't
improve performance much.

### The Super Learner Algorithm

We can combine all of the above elements, `Pipeline`s, `Stack`s, and
cross-validation using `Lrnr_cv`, to easily define a Super Learner. The Super
Learner algorithm works by fitting a "meta-learner", which combines predictions
from multiple stacked learners. It does this while avoiding overfitting by
training the meta-learner on validation-set predictions in a manner that is
cross-validated. Using some of the objects we defined in the above examples,
this becomes a very simple operation:

```{r sl3-metalearner-glm}
metalearner <- make_learner(Lrnr_nnls)
cv_task <- cv_fit$chain()
ml_fit <- metalearner$train(cv_task)
```

Here, we used a special learner, `Lrnr_nnls`, for the meta-learning step. This
fits a non-negative least squares meta-learner. It is important to note that any
learner can be used as a meta-learner.

The Super Learner finally produced is defined as a pipeline with the learner
stack trained on the full data and the meta-learner trained on the
validation-set predictions. Below, we use a special behavior of pipelines: if
all objects passed to a pipeline are learner fits (i.e., `learner$is_trained` is
`TRUE`), the result will also be a fit:

```{r sl3-define-SuperLearner}
sl_pipeline <- make_learner(Pipeline, stack_fit, ml_fit)
sl_preds <- sl_pipeline$predict()
head(sl_preds)
```

A Super Learner may be fit in a more streamlined manner using the `Lrnr_sl`
learner. For simplicity, we will use the same set of learners and meta-learning
algorithm as we did before:

```{r sl3-Lrnr_sl}
sl <- Lrnr_sl$new(learners = stack,
                  metalearner = metalearner)
sl_fit <- sl$train(task)
lrnr_sl_preds <- sl_fit$predict()
head(lrnr_sl_preds)
```

We can see that this generates the same predictions as the more hands-on
definition above.

## Computation with `delayed`

Fitting a Super Learner is composed of many different training and prediction
steps, as the procedure requires that the learners in the stack and the
meta-learner be fit on cross-validation folds and on the full data. For large
datasets, this can be extremely time-consuming. To alleviate this complication,
we've developed a specialized parallelization framework `delayed` that
parallelizes across these tasks in a way that takes into account their
inter-dependent nature. Consider a Super Learner with three learners:

```{r sl3-delayed-sl}
lrnr_rf <- make_learner(Lrnr_randomForest)
lrnr_glmnet <- make_learner(Lrnr_glmnet)
sl <- Lrnr_sl$new(learners = list(lrnr_glm, lrnr_rf, lrnr_glmnet),
                  metalearner = metalearner)
```

We can plot the network of tasks required to train this Super Learner:

```{r sl3-delayed-plot}
delayed_sl_fit <- delayed_learner_train(sl, task)
plot(delayed_sl_fit)
```

`delayed` then allows us to parallelize the procedure across these tasks using
the [`future`](https://github.com/HenrikBengtsson/future) package. For more
information on specifying `future` `plan`s for parallelization, see the
documentation of the [`future`](https://github.com/HenrikBengtsson/future)
package. Performance comparisons can be found in the ["SuperLearner
Benchmarks"](https://sl3.tlverse.org/articles/SuperLearner_benchmarks.html) vignette that accompanies this
package. This feature is currently experimental and hasn't yet been throughly
tested on a range of parallel backends.

---

## Session Information

```{r sessionInfo, echo=FALSE}
sessionInfo()
```

---

## References

